{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "seminar.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.12"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_lhrn5O-qUYZ"
      },
      "source": [
        "# Import and misc"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "meO-Mp9jiAFC",
        "outputId": "3ce0c838-cd55-4199-e590-2116cd25d35f"
      },
      "source": [
        "!pip install torchaudio==0.9.1"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torchaudio==0.9.1\n",
            "  Downloading torchaudio-0.9.1-cp37-cp37m-manylinux1_x86_64.whl (1.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.9 MB 5.4 MB/s \n",
            "\u001b[?25hCollecting torch==1.9.1\n",
            "  Downloading torch-1.9.1-cp37-cp37m-manylinux1_x86_64.whl (831.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 831.4 MB 6.8 kB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch==1.9.1->torchaudio==0.9.1) (3.7.4.3)\n",
            "Installing collected packages: torch, torchaudio\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 1.9.0+cu111\n",
            "    Uninstalling torch-1.9.0+cu111:\n",
            "      Successfully uninstalled torch-1.9.0+cu111\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchvision 0.10.0+cu111 requires torch==1.9.0, but you have torch 1.9.1 which is incompatible.\n",
            "torchtext 0.10.0 requires torch==1.9.0, but you have torch 1.9.1 which is incompatible.\u001b[0m\n",
            "Successfully installed torch-1.9.1 torchaudio-0.9.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bbUpoArCqUYa"
      },
      "source": [
        "from typing import Tuple\n",
        "from tqdm import tqdm\n",
        "from itertools import islice\n",
        "import dataclasses\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch import nn\n",
        "from torch import distributions\n",
        "from torch.utils.data import DataLoader, Dataset, WeightedRandomSampler\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "\n",
        "import torchaudio\n",
        "from IPython import display as display_"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5fwUK9epqUYh"
      },
      "source": [
        "def count_parameters(model):\n",
        "    return sum([p.numel() for p in model.parameters() if p.requires_grad])"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "812GwLfqqUYf"
      },
      "source": [
        "# Task"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i1DuQIyRqUYf"
      },
      "source": [
        "In this notebook we will implement a model for finding a keyword in a stream.\n",
        "\n",
        "We will implement the version with CRNN because it is easy and improves the model. \n",
        "(from https://www.dropbox.com/s/22ah2ba7dug6pzw/KWS_Attention.pdf)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8PdhApeEh9pH"
      },
      "source": [
        "@dataclasses.dataclass\n",
        "class TaskConfig:\n",
        "    keyword: str = 'sheila'  # We will use 1 key word -- 'sheila'\n",
        "    batch_size: int = 256\n",
        "    learning_rate: float = 3e-4\n",
        "    weight_decay: float = 1e-5\n",
        "    num_epochs: int = 35\n",
        "    n_mels: int = 40\n",
        "    kernel_size: Tuple[int, int] = (20, 5)\n",
        "    stride: Tuple[int, int] = (8, 2)\n",
        "    hidden_size: int = 128\n",
        "    gru_num_layers: int = 2\n",
        "    bidirectional: bool = True\n",
        "    num_classes: int = 2\n",
        "    sample_rate: int = 16000\n",
        "    device: torch.device = torch.device(\n",
        "        'cuda:0' if torch.cuda.is_available() else 'cpu')"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KA1gPmE1h9pI"
      },
      "source": [
        "# Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dhDvVqcZh9pI",
        "outputId": "5fb1e87d-51ba-447d-f6b7-058e0360304c"
      },
      "source": [
        "!wget https://gist.githubusercontent.com/Kirili4ik/6ac5c745ff8dad094e9c464c08f66f3e/raw/63daacc17f52a7d90f7f4166a3f5deef62b165db/dataset_utils.py"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2021-11-07 15:06:42--  https://gist.githubusercontent.com/Kirili4ik/6ac5c745ff8dad094e9c464c08f66f3e/raw/63daacc17f52a7d90f7f4166a3f5deef62b165db/dataset_utils.py\n",
            "Resolving gist.githubusercontent.com (gist.githubusercontent.com)... 185.199.111.133, 185.199.110.133, 185.199.109.133, ...\n",
            "Connecting to gist.githubusercontent.com (gist.githubusercontent.com)|185.199.111.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 3475 (3.4K) [text/plain]\n",
            "Saving to: ‘dataset_utils.py’\n",
            "\n",
            "\rdataset_utils.py      0%[                    ]       0  --.-KB/s               \rdataset_utils.py    100%[===================>]   3.39K  --.-KB/s    in 0s      \n",
            "\n",
            "2021-11-07 15:06:42 (34.2 MB/s) - ‘dataset_utils.py’ saved [3475/3475]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 252
        },
        "id": "QKgGdL8jh9pJ",
        "outputId": "faf10e67-60ce-4de9-f97a-39bcc81bfd24"
      },
      "source": [
        "from dataset_utils import DatasetDownloader, TrainDataset\n",
        "\n",
        "dataset_downloader = DatasetDownloader(TaskConfig.keyword)\n",
        "labeled_data, _ = dataset_downloader.generate_labeled_data()\n",
        "\n",
        "labeled_data.sample(3)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data...\n",
            "Ready!\n",
            "Classes: bed, bird, cat, dog, down, eight, five, four, go, happy, house, left, marvin, nine, no, off, on, one, right, seven, sheila, six, stop, three, tree, two, up, wow, yes, zero\n",
            "Creating labeled dataframe:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 31/31 [06:25<00:00, 12.44s/it]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>name</th>\n",
              "      <th>word</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>46789</th>\n",
              "      <td>speech_commands/house/257e17e0_nohash_0.wav</td>\n",
              "      <td>house</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24551</th>\n",
              "      <td>speech_commands/right/26b28ea7_nohash_0.wav</td>\n",
              "      <td>right</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29539</th>\n",
              "      <td>speech_commands/go/bfb10243_nohash_0.wav</td>\n",
              "      <td>go</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                              name   word label\n",
              "46789  speech_commands/house/257e17e0_nohash_0.wav  house     0\n",
              "24551  speech_commands/right/26b28ea7_nohash_0.wav  right     0\n",
              "29539     speech_commands/go/bfb10243_nohash_0.wav     go     0"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LUxfDJw1qUYi"
      },
      "source": [
        "### Augmentations"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dkmkxPWQqUYe"
      },
      "source": [
        "class AugsCreation:\n",
        "\n",
        "    def __init__(self):\n",
        "        self.background_noises = [\n",
        "            'speech_commands/_background_noise_/white_noise.wav',\n",
        "            'speech_commands/_background_noise_/dude_miaowing.wav',\n",
        "            'speech_commands/_background_noise_/doing_the_dishes.wav',\n",
        "            'speech_commands/_background_noise_/exercise_bike.wav',\n",
        "            'speech_commands/_background_noise_/pink_noise.wav',\n",
        "            'speech_commands/_background_noise_/running_tap.wav'\n",
        "        ]\n",
        "\n",
        "    def add_rand_noise(self, audio):\n",
        "\n",
        "        # randomly choose noise\n",
        "        noise_num = torch.randint(low=0, high=len(\n",
        "            self.background_noises), size=(1,)).item()\n",
        "        noise = torchaudio.load(self.background_noises[noise_num])[0].squeeze()\n",
        "\n",
        "        noise_level = torch.Tensor([1])  # [0, 40]\n",
        "\n",
        "        noise_energy = torch.norm(noise)\n",
        "        audio_energy = torch.norm(audio)\n",
        "        alpha = (audio_energy / noise_energy) * \\\n",
        "            torch.pow(10, -noise_level / 20)\n",
        "\n",
        "        start = torch.randint(low=0, high=int(\n",
        "            noise.size(0) - audio.size(0) - 1), size=(1,)).item()\n",
        "        noise_sample = noise[start: start + audio.size(0)]\n",
        "\n",
        "        audio_new = audio + alpha * noise_sample\n",
        "        audio_new.clamp_(-1, 1)\n",
        "        return audio_new\n",
        "\n",
        "    def __call__(self, wav):\n",
        "        aug_num = torch.randint(low=0, high=4, size=(1,)).item()   # choose 1 random aug from augs\n",
        "        augs = [\n",
        "            lambda x: x,\n",
        "            lambda x: (x + distributions.Normal(0, 0.01).sample(x.size())).clamp_(-1, 1),\n",
        "            lambda x: torchaudio.transforms.Vol(.25)(x),\n",
        "            lambda x: self.add_rand_noise(x)\n",
        "        ]\n",
        "\n",
        "        return augs[aug_num](wav)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ClWThxyYh9pM"
      },
      "source": [
        "indexes = torch.randperm(len(labeled_data))\n",
        "train_indexes = indexes[:int(len(labeled_data) * 0.8)]\n",
        "val_indexes = indexes[int(len(labeled_data) * 0.8):]\n",
        "\n",
        "train_df = labeled_data.iloc[train_indexes].reset_index(drop=True)\n",
        "val_df = labeled_data.iloc[val_indexes].reset_index(drop=True)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PDPLht5fqUYe"
      },
      "source": [
        "# Sample is a dict of utt, word and label\n",
        "transform_tr = AugsCreation()\n",
        "train_set = TrainDataset(df=train_df, kw=TaskConfig.keyword, transform=transform_tr)\n",
        "val_set = TrainDataset(df=val_df, kw=TaskConfig.keyword)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2vbPDqd6qUYj"
      },
      "source": [
        "### Sampler for oversampling:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rfnjRKo2qUYj"
      },
      "source": [
        "# We should provide to WeightedRandomSampler _weight for every sample_; by default it is 1/len(target)\n",
        "\n",
        "def get_sampler(target):\n",
        "    class_sample_count = np.array(\n",
        "        [len(np.where(target == t)[0]) for t in np.unique(target)])   # for every class count it's number of occ.\n",
        "    weight = 1. / class_sample_count\n",
        "    samples_weight = np.array([weight[t] for t in target])\n",
        "    samples_weight = torch.from_numpy(samples_weight)\n",
        "    samples_weigth = samples_weight.double()\n",
        "    sampler = WeightedRandomSampler(samples_weight, len(samples_weight))\n",
        "    return sampler"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UM8gLmHeqUYj"
      },
      "source": [
        "train_sampler = get_sampler(train_set.df['label'].values)\n",
        "val_sampler = get_sampler(val_set.df['label'].values)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lyBqbxp0h9pO"
      },
      "source": [
        "class Collator:\n",
        "    \n",
        "    def __call__(self, data):\n",
        "        wavs = []\n",
        "        labels = []    \n",
        "\n",
        "        for el in data:\n",
        "            wavs.append(el['utt'])\n",
        "            labels.append(el['label'])\n",
        "\n",
        "        # torch.nn.utils.rnn.pad_sequence takes list(Tensors) and returns padded (with 0.0) Tensor\n",
        "        wavs = pad_sequence(wavs, batch_first=True)    \n",
        "        labels = torch.Tensor(labels).long()\n",
        "        return wavs, labels"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e8G9xPRVqUYk"
      },
      "source": [
        "###  Dataloaders"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6wGBMcQiqUYk"
      },
      "source": [
        "# Here we are obliged to use shuffle=False because of our sampler with randomness inside.\n",
        "\n",
        "train_loader = DataLoader(train_set, batch_size=TaskConfig.batch_size,\n",
        "                          shuffle=False, collate_fn=Collator(),\n",
        "                          sampler=train_sampler)\n",
        "#                           num_workers=2, pin_memory=True)\n",
        "\n",
        "val_loader = DataLoader(val_set, batch_size=TaskConfig.batch_size,\n",
        "                        shuffle=False, collate_fn=Collator(),\n",
        "                        sampler=val_sampler,\n",
        "                        num_workers=2, pin_memory=True)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kTlsn6cpqUYk"
      },
      "source": [
        "### Creating MelSpecs on GPU for speeeed: "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pRXMt6it56fW"
      },
      "source": [
        "class LogMelspec():\n",
        "\n",
        "    def __init__(self, is_train, config):\n",
        "        # with augmentations\n",
        "        if is_train:\n",
        "            self.melspec = nn.Sequential(\n",
        "                torchaudio.transforms.MelSpectrogram(\n",
        "                    sample_rate=config.sample_rate,  n_mels=config.n_mels),\n",
        "                torchaudio.transforms.FrequencyMasking(freq_mask_param=15),\n",
        "                torchaudio.transforms.TimeMasking(time_mask_param=35),\n",
        "            ).to(config.device)\n",
        "\n",
        "        # no augmentations\n",
        "        else:\n",
        "            self.melspec = torchaudio.transforms.MelSpectrogram(\n",
        "                sample_rate=config.sample_rate,\n",
        "                n_mels=config.n_mels\n",
        "            ).to(config.device)\n",
        "\n",
        "    def __call__(self, batch):\n",
        "        # already on device\n",
        "        return torch.log(self.melspec(batch).clamp_(min=1e-9, max=1e9))"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pqkz4_gn8BiF"
      },
      "source": [
        "melspec_train = LogMelspec(is_train=True, config=TaskConfig)\n",
        "melspec_val = LogMelspec(is_train=False, config=TaskConfig)"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zoAxmihY8yxr"
      },
      "source": [
        "### Quality measurment functions:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "euwD1UyuqUYk"
      },
      "source": [
        "# FA - true: 0, model: 1\n",
        "# FR - true: 1, model: 0\n",
        "\n",
        "def count_FA_FR(preds, labels):\n",
        "    FA = torch.sum(preds[labels == 0])\n",
        "    FR = torch.sum(labels[preds == 0])\n",
        "    \n",
        "    # torch.numel - returns total number of elements in tensor\n",
        "    return FA.item() / torch.numel(preds), FR.item() / torch.numel(preds)"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YHBUrkT1qUYk"
      },
      "source": [
        "def get_au_fa_fr(probs, labels):\n",
        "    sorted_probs, _ = torch.sort(probs)\n",
        "    sorted_probs = torch.cat((torch.Tensor([0]), sorted_probs, torch.Tensor([1])))\n",
        "    labels = torch.cat(labels, dim=0)\n",
        "        \n",
        "    FAs, FRs = [], []\n",
        "    for prob in sorted_probs:\n",
        "        preds = (probs >= prob) * 1\n",
        "        FA, FR = count_FA_FR(preds, labels)        \n",
        "        FAs.append(FA)\n",
        "        FRs.append(FR)\n",
        "    # plt.plot(FAs, FRs)\n",
        "    # plt.show()\n",
        "\n",
        "    # ~ area under curve using trapezoidal rule\n",
        "    return -np.trapz(FRs, x=FAs)"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CcEP5cEZqUYl"
      },
      "source": [
        "### Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "26_QtIlXqUYl"
      },
      "source": [
        "# Pay attention to _groups_ param\n",
        "\n",
        "def SepConv(in_size, out_size, kernel_size, stride, padding=0):\n",
        "    return nn.Sequential(\n",
        "        torch.nn.Conv1d(in_size, in_size, kernel_size[1],\n",
        "                        stride=stride[1], groups=in_size,\n",
        "                        padding=padding),\n",
        "\n",
        "        torch.nn.Conv1d(in_size, out_size, kernel_size=1,\n",
        "                        stride=stride[0], groups=int(in_size / kernel_size[0]))\n",
        "    )"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UIT6STF_qUYl"
      },
      "source": [
        "class CRNN(nn.Module):\n",
        "    \n",
        "    def __init__(self, config):\n",
        "        super(CRNN, self).__init__()\n",
        "\n",
        "        self.sepconv = SepConv(in_size=config.n_mels, out_size=config.hidden_size,\n",
        "                               kernel_size=config.kernel_size, stride=config.stride)\n",
        "\n",
        "        self.gru = nn.GRU(input_size=config.hidden_size, hidden_size=config.hidden_size,\n",
        "                          num_layers=config.gru_num_layers,\n",
        "                          dropout=0.1,\n",
        "                          bidirectional=config.bidirectional)\n",
        "\n",
        "    def forward(self, x, hidden):\n",
        "        x = self.sepconv(x)\n",
        "\n",
        "        # (BS, hidden, seq_len) ->(seq_len, BS, hidden)\n",
        "        x = x.permute(2, 0, 1)\n",
        "        x, hidden = self.gru(x, hidden)\n",
        "        # x : (seq_len, BS, hidden * num_dirs)\n",
        "        # hidden : (num_layers * num_dirs, BS, hidden)\n",
        "\n",
        "        return x, hidden"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nHZTwEdyqUYl"
      },
      "source": [
        "class AttnMech(nn.Module):\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super(AttnMech, self).__init__()\n",
        "\n",
        "        ratio = 2 if config.bidirectional else 1\n",
        "        lin_size = config.hidden_size * ratio\n",
        "\n",
        "        self.Wx_b = nn.Linear(lin_size, lin_size)\n",
        "        self.Vt = nn.Linear(lin_size, 1, bias=False)\n",
        "\n",
        "    def forward(self, inputs, data=None):\n",
        "\n",
        "        # count only 1 e_t\n",
        "        if data is None:\n",
        "            x = inputs\n",
        "            x = torch.tanh(self.Wx_b(x))\n",
        "            e = self.Vt(x)\n",
        "            return e\n",
        "\n",
        "        # recount attention for full vector e\n",
        "        e = inputs\n",
        "        # (BS, seq_len, hid_size*num_dirs)\n",
        "        data = data.transpose(0, 1)\n",
        "        alphas = F.softmax(e, dim=-1).unsqueeze(1)\n",
        "        c = torch.matmul(alphas, data).squeeze()   # attetntion_vector\n",
        "        return c"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5BkVo6qpqUYm"
      },
      "source": [
        "class FullModel(nn.Module):\n",
        "\n",
        "    def __init__(self, config, CRNN_model, attn_layer):\n",
        "        super(FullModel, self).__init__()\n",
        "\n",
        "        self.CRNN_model = CRNN_model\n",
        "        self.attn_layer = attn_layer\n",
        "\n",
        "        # ll_in_size, ll_out_size = HIDDEN_SIZE * GRU_NUM_DIRS, NUM_CLASSES\n",
        "        # last layer\n",
        "        ratio = 2 if config.bidirectional else 1\n",
        "        self.U = nn.Linear(config.hidden_size * ratio,\n",
        "                           config.num_classes, bias=False)\n",
        "\n",
        "    def forward(self, batch, hidden=None):\n",
        "        output, hidden = self.CRNN_model(batch, hidden)\n",
        "        # output : (seq_len, BS, hidden * num_dirs)\n",
        "        # hidden : (num_layers * num_dirs, BS, hidden)\n",
        "\n",
        "        e = []\n",
        "        for seq_el in output:\n",
        "            e_t = self.attn_layer(seq_el)  # (BS, 1)\n",
        "            e.append(e_t)\n",
        "        e = torch.cat(e, dim=1)           # (BS, seq_len)\n",
        "\n",
        "        c = self.attn_layer(e, output)    # attention_vector\n",
        "        Uc = self.U(c)\n",
        "        return Uc               # we will need to get probs, so we use return logits"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DmmSFvWaqUYn"
      },
      "source": [
        "def train_epoch(model, opt, loader, log_melspec, device):\n",
        "    model.train()\n",
        "    for i, (batch, labels) in tqdm(enumerate(loader)):\n",
        "        batch, labels = batch.to(device), labels.to(device)\n",
        "        batch = log_melspec(batch)\n",
        "\n",
        "        opt.zero_grad()\n",
        "\n",
        "        # run model # with autocast():\n",
        "        logits = model(batch)\n",
        "        # we need probabilities so we use softmax & CE separately\n",
        "        probs = F.softmax(logits, dim=-1)\n",
        "        loss = F.cross_entropy(logits, labels)\n",
        "\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 5)\n",
        "\n",
        "        opt.step()\n",
        "\n",
        "        # logging\n",
        "        argmax_probs = torch.argmax(probs, dim=-1)\n",
        "        FA, FR = count_FA_FR(argmax_probs, labels)\n",
        "        acc = torch.sum(argmax_probs == labels) / torch.numel(argmax_probs)\n",
        "\n",
        "    return acc"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UIeRbn4tqUYo"
      },
      "source": [
        "@torch.no_grad()\n",
        "def validation(model, loader, log_melspec, device):\n",
        "    model.eval()\n",
        "\n",
        "    val_losses, accs, FAs, FRs = [], [], [], []\n",
        "    all_probs, all_labels = [], []\n",
        "    for i, (batch, labels) in tqdm(enumerate(loader)):\n",
        "        batch, labels = batch.to(device), labels.to(device)\n",
        "        batch = log_melspec(batch)\n",
        "\n",
        "        output = model(batch)\n",
        "        # we need probabilities so we use softmax & CE separately\n",
        "        probs = F.softmax(output, dim=-1)\n",
        "        loss = F.cross_entropy(output, labels)\n",
        "\n",
        "        # logging\n",
        "        argmax_probs = torch.argmax(probs, dim=-1)\n",
        "        all_probs.append(probs[:, 1].cpu())\n",
        "        all_labels.append(labels.cpu())\n",
        "        val_losses.append(loss.item())\n",
        "        accs.append(\n",
        "            torch.sum(argmax_probs == labels).item() /  # ???\n",
        "            torch.numel(argmax_probs)\n",
        "        )\n",
        "        FA, FR = count_FA_FR(argmax_probs, labels)\n",
        "        FAs.append(FA)\n",
        "        FRs.append(FR)\n",
        "\n",
        "    # area under FA/FR curve for whole loader\n",
        "    au_fa_fr = get_au_fa_fr(torch.cat(all_probs, dim=0).cpu(), all_labels)\n",
        "    return au_fa_fr"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PpyvKwp0k3IU"
      },
      "source": [
        "from collections import defaultdict\n",
        "from IPython.display import clear_output\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "history = defaultdict(list)"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q8sVpHNoocgA",
        "outputId": "91f2f178-98a6-448c-be99-80457a76ee4a"
      },
      "source": [
        "CRNN_model = CRNN(TaskConfig)\n",
        "attn_layer = AttnMech(TaskConfig)\n",
        "full_model = FullModel(TaskConfig, CRNN_model, attn_layer)\n",
        "full_model = full_model.to(TaskConfig.device)\n",
        "\n",
        "print(full_model)\n",
        "\n",
        "opt = torch.optim.Adam(full_model.parameters(),\n",
        "                       lr=TaskConfig.learning_rate, weight_decay=TaskConfig.weight_decay)"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "FullModel(\n",
            "  (CRNN_model): CRNN(\n",
            "    (sepconv): Sequential(\n",
            "      (0): Conv1d(40, 40, kernel_size=(5,), stride=(2,), groups=40)\n",
            "      (1): Conv1d(40, 128, kernel_size=(1,), stride=(8,), groups=2)\n",
            "    )\n",
            "    (gru): GRU(128, 128, num_layers=2, dropout=0.1, bidirectional=True)\n",
            "  )\n",
            "  (attn_layer): AttnMech(\n",
            "    (Wx_b): Linear(in_features=256, out_features=256, bias=True)\n",
            "    (Vt): Linear(in_features=256, out_features=1, bias=False)\n",
            "  )\n",
            "  (U): Linear(in_features=256, out_features=2, bias=False)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aZ3sF--l3tNR"
      },
      "source": [
        ""
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H5lGIOOx8DHq"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        },
        "id": "32oooz4lqUYo",
        "scrolled": false,
        "outputId": "c4fdfc31-03fa-4540-9c03-4d72c6bdf714"
      },
      "source": [
        "# TRAIN\n",
        "\n",
        "for n in range(15):\n",
        "\n",
        "    train_epoch(full_model, opt, train_loader,\n",
        "                melspec_train, TaskConfig.device)\n",
        "\n",
        "    au_fa_fr = validation(full_model, val_loader,\n",
        "                          melspec_val, TaskConfig.device)\n",
        "    history['val_metric'].append(au_fa_fr)\n",
        "\n",
        "    clear_output()\n",
        "    plt.plot(history['val_metric'])\n",
        "    plt.ylabel('Metric')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.grid()\n",
        "    plt.show()\n",
        "\n",
        "    print('END OF EPOCH', n)"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "END OF EPOCH 14\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t55FUkOGh9pT"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}